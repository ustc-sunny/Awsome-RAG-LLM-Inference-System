# Awsome-RAG-LLM-Inference-System
A survey about RAG-based LLM inference: a survey about systematic designs

## 1. GPU Accelerate
* [RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving ](https://arxiv.org/abs/2503.14649), ISCA 2025
* [Hermes: Algorithm-System Co-design for Efficient Retrieval Augmented Generation At Scale](https://michaeltshen.github.io/Files/Hermes.pdf), ISCA 2025
* [Fast Vector Query Processing for Large Datasets Beyond GPU Memory with Reordered Pipelining](https://xinjin.github.io/files/NSDI24_Rummy.pdf), NSDI 2024

## 2. Process in Memory

## 3. Hardware Accelerate (Near memory)
* [Accelerating Retrieval-Augmented Generation](https://dl.acm.org/doi/10.1145/3669940.3707264), ASPLOS 2025
